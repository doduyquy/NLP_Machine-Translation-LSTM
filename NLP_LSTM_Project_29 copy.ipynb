{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Y5-hTNuMCR",
        "outputId": "fe610795-e0c8-4324-f7f9-c5e284a8b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (3.9.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.12.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: click in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:12\n",
            "      --------------------------------------- 0.3/12.8 MB 2.6 MB/s eta 0:00:05\n",
            "     - -------------------------------------- 0.6/12.8 MB 4.0 MB/s eta 0:00:04\n",
            "     -- ------------------------------------- 0.9/12.8 MB 4.6 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.5/12.8 MB 5.9 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 6.4 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.3/12.8 MB 6.5 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 2.6/12.8 MB 6.7 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.1/12.8 MB 7.0 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 7.3 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 4.1/12.8 MB 7.0 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.4/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.4/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 4.4/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.4/12.8 MB 7.1 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 5.8/12.8 MB 7.1 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 6.2/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 6.6/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 7.0/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.4/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 7.9/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 8.2/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.4/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.7/12.8 MB 7.3 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.9/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.6/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.8/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.2/12.8 MB 7.2 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 10.5/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 10.9/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 11.1/12.8 MB 7.5 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.4/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 11.9/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.5/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.8/12.8 MB 7.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 7.2 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/16.3 MB 3.3 MB/s eta 0:00:05\n",
            "      --------------------------------------- 0.4/16.3 MB 4.5 MB/s eta 0:00:04\n",
            "     - -------------------------------------- 0.8/16.3 MB 5.3 MB/s eta 0:00:03\n",
            "     -- ------------------------------------- 1.2/16.3 MB 6.5 MB/s eta 0:00:03\n",
            "     --- ------------------------------------ 1.6/16.3 MB 6.9 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 2.0/16.3 MB 7.1 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 2.4/16.3 MB 7.4 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 2.8/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 3.2/16.3 MB 7.6 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 3.6/16.3 MB 7.7 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 4.0/16.3 MB 7.8 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 4.5/16.3 MB 7.9 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 4.8/16.3 MB 7.8 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 5.0/16.3 MB 7.6 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 5.3/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 5.5/16.3 MB 7.3 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 5.9/16.3 MB 7.4 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 6.4/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 6.7/16.3 MB 7.6 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 7.1/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 7.3/16.3 MB 7.4 MB/s eta 0:00:02\n",
            "     ------------------ --------------------- 7.6/16.3 MB 7.3 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 7.8/16.3 MB 7.2 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 8.0/16.3 MB 7.2 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 8.3/16.3 MB 7.1 MB/s eta 0:00:02\n",
            "     -------------------- ------------------- 8.5/16.3 MB 7.0 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 8.8/16.3 MB 6.9 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 9.1/16.3 MB 6.9 MB/s eta 0:00:02\n",
            "     ---------------------- ----------------- 9.3/16.3 MB 6.8 MB/s eta 0:00:02\n",
            "     ----------------------- ---------------- 9.5/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 9.8/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 10.2/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 10.7/16.3 MB 7.0 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 10.9/16.3 MB 7.0 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 11.0/16.3 MB 6.9 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 11.3/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 11.8/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 12.1/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 12.6/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 13.0/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 13.4/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 13.8/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 14.2/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 14.6/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 14.9/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 15.1/16.3 MB 6.6 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 15.5/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ---------------------------------------  15.9/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------------  16.3/16.3 MB 6.9 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 16.3/16.3 MB 6.5 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy nltk --user\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_L-aldtkrB",
        "outputId": "55e1da60-97ab-44c3-e62f-a9b8751a5ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import random\n",
        "# from numba import cuda\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2rjV_WtJE0_",
        "outputId": "f86fe5cb-56ed-4b02-bdee-4c5b7ef012e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 29000\n",
            "Val examples: 1014\n",
            "Test examples: 1071\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. LOAD DATA (TRAIN / VAL / TEST)\n",
        "# ============================================================\n",
        "\n",
        "def load_file(path):\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "# train_en = load_file(\"/content/train.en\")\n",
        "# train_fr = load_file(\"/content/train.fr\")\n",
        "\n",
        "# val_en = load_file(\"/content/val.en\")\n",
        "# val_fr = load_file(\"/content/val.fr\")\n",
        "\n",
        "# test_en = load_file(\"/content/test.en\")\n",
        "# test_fr = load_file(\"/content/test.fr\")\n",
        "\n",
        "train_en = load_file(\"./data/train.en\")\n",
        "train_fr = load_file(\"./data/train.fr\")\n",
        "\n",
        "val_en = load_file(\"./data/val.en\")\n",
        "val_fr = load_file(\"./data/val.fr\")\n",
        "\n",
        "test_en = load_file(\"./data/test.en\")\n",
        "test_fr = load_file(\"./data/test.fr\")\n",
        "\n",
        "print(\"Train examples:\", len(train_en))\n",
        "print(\"Val examples:\", len(val_en))\n",
        "print(\"Test examples:\", len(test_en))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "biwtBbHyK-xZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. TOKENIZATION (spaCy)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_en.tokenizer(text)]\n",
        "def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_fr.tokenizer(text)]\n",
        "\n",
        "train_en_tok = [en_tokenizer(s) for s in train_en]\n",
        "train_fr_tok = [fr_tokenizer(s) for s in train_fr]\n",
        "\n",
        "val_en_tok = [en_tokenizer(s) for s in val_en]\n",
        "val_fr_tok = [fr_tokenizer(s) for s in val_fr]\n",
        "\n",
        "test_en_tok = [en_tokenizer(s) for s in test_en]\n",
        "test_fr_tok = [fr_tokenizer(s) for s in test_fr]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-dQoFk3uW-mq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. BUILD VOCAB\n",
        "# ============================================================\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, max_size=10000):\n",
        "        self.freq = Counter(tokens)\n",
        "        most_common = self.freq.most_common(max_size)\n",
        "\n",
        "        # word2idx\n",
        "        self.itos = SPECIAL_TOKENS + [w for w, _ in most_common]\n",
        "        self.stoi = {w:i for i, w in enumerate(self.itos)}\n",
        "\n",
        "    def numericalize(self, tokens):\n",
        "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "en_vocab = Vocab(tok for sent in train_en_tok for tok in sent)\n",
        "fr_vocab = Vocab(tok for sent in train_fr_tok for tok in sent)\n",
        "\n",
        "PAD_IDX = fr_vocab.stoi[\"<pad>\"]\n",
        "SOS_IDX = fr_vocab.stoi[\"<sos>\"]\n",
        "EOS_IDX = fr_vocab.stoi[\"<eos>\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3aXkGj5CagMq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. DATASET + DATALOADER\n",
        "# ============================================================\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en_data, fr_data, vocab_en, vocab_fr):\n",
        "        self.en = en_data\n",
        "        self.fr = fr_data\n",
        "        self.vocab_en = vocab_en\n",
        "        self.vocab_fr = vocab_fr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "    # def numericalize(self, tokens, vocab):\n",
        "    #     return [vocab[\"<sos>\"]] + [vocab[t] for t in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     src_num = self.numericalize(self.src[idx], self.src_vocab)\n",
        "    #     trg_num = self.numericalize(self.trg[idx], self.trg_vocab)\n",
        "    #     return torch.tensor(src_num), torch.tensor(trg_num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_tokens = [\"<sos>\"] + self.en[idx] + [\"<eos>\"]\n",
        "        fr_tokens = [\"<sos>\"] + self.fr[idx] + [\"<eos>\"]\n",
        "\n",
        "        en_ids = self.vocab_en.numericalize(en_tokens)\n",
        "        fr_ids = self.vocab_fr.numericalize(fr_tokens)\n",
        "\n",
        "        return torch.tensor(en_ids), torch.tensor(fr_ids)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    en_batch, fr_batch = zip(*batch)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    return en_batch, fr_batch\n",
        "\n",
        "\n",
        "train_ds = TranslationDataset(train_en_tok, train_fr_tok, en_vocab, fr_vocab)\n",
        "val_ds = TranslationDataset(val_en_tok, val_fr_tok, en_vocab, fr_vocab)\n",
        "test_ds = TranslationDataset(test_en_tok, test_fr_tok, en_vocab, fr_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iBKoX7BLcpDe"
      },
      "outputs": [],
      "source": [
        "# # ============================================================\n",
        "# # 6. ENCODER - DECODER MODEL\n",
        "# # ============================================================\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "#         self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "#                             num_layers=num_layers,\n",
        "#                             dropout=dropout)\n",
        "\n",
        "#     def forward(self, src):\n",
        "#         embedded = self.embedding(src)\n",
        "#         outputs, (hidden, cell) = self.lstm(embedded)\n",
        "#         return hidden, cell\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "#         self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "#                             num_layers=num_layers,\n",
        "#                             dropout=dropout)\n",
        "#         self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "#     def forward(self, input, hidden, cell):\n",
        "#         input = input.unsqueeze(0)\n",
        "#         embedded = self.embedding(input)\n",
        "#         output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "#         prediction = self.fc(output.squeeze(0))\n",
        "#         return prediction, hidden, cell\n",
        "\n",
        "\n",
        "# class Seq2Seq(nn.Module):\n",
        "#     def __init__(self, encoder, decoder):\n",
        "#         super().__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "\n",
        "#     def forward(self, src, trg, teacher_forcing=0.3):\n",
        "#         batch_size = trg.size(1)\n",
        "#         max_len = trg.size(0)\n",
        "#         vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "#         outputs = torch.zeros(max_len, batch_size, vocab_size).to(src.device)\n",
        "\n",
        "#         hidden, cell = self.encoder(src)\n",
        "#         input_token = trg[0, :]\n",
        "\n",
        "#         for t in range(1, max_len):\n",
        "#             output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "#             outputs[t] = output\n",
        "#             best = output.argmax(1)\n",
        "\n",
        "#             input_token = trg[t] if random.random() < teacher_forcing else best\n",
        "\n",
        "#         return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        decoder_hidden: [num_layers, batch, hidden]\n",
        "        encoder_outputs: [src_len, batch, hidden]\n",
        "        \"\"\"\n",
        "        # lấy hidden của layer cuối: [batch, hidden]\n",
        "        decoder_hidden = decoder_hidden[-1].unsqueeze(2)  # [batch, hidden, 1]\n",
        "\n",
        "        # Score = encoder_output · decoder_hidden\n",
        "        # encoder_outputs: [src_len, batch, hidden]\n",
        "        # sau permute:    [batch, src_len, hidden]\n",
        "        scores = torch.bmm(\n",
        "            encoder_outputs.permute(1,0,2),\n",
        "            decoder_hidden\n",
        "        ).squeeze(2)  # [batch, src_len]\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=1)  # [batch, src_len]\n",
        "\n",
        "        return attn_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell   # trả về tất cả hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "        \n",
        "        self.attention = LuongAttention(hidden_dim)\n",
        "\n",
        "        # combine context + decoder hidden\n",
        "        self.fc_concat = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
        "        input_token = input_token.unsqueeze(0)\n",
        "        embedded = self.embedding(input_token)\n",
        "\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        lstm_output = lstm_output.squeeze(0)  # [batch, hidden]\n",
        "\n",
        "        # =========== Luong Attention ============\n",
        "        attn_weights = self.attention(hidden, encoder_outputs)  # [batch, src_len]\n",
        "\n",
        "        # context vector = sum(attn * encoder_outputs)\n",
        "        context = torch.bmm(\n",
        "            attn_weights.unsqueeze(1),       # [batch, 1, src_len]\n",
        "            encoder_outputs.permute(1,0,2)   # [batch, src_len, hidden]\n",
        "        ).squeeze(1)  # [batch, hidden]\n",
        "\n",
        "        # concat context + output\n",
        "        combined = torch.cat((lstm_output, context), dim=1)  # [batch, 2*hidden]\n",
        "        combined = torch.tanh(self.fc_concat(combined))      # [batch, hidden]\n",
        "\n",
        "        # final prediction\n",
        "        prediction = self.fc_out(combined)  # [batch, vocab]\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing=0.3):\n",
        "        batch_size = trg.size(1)\n",
        "        max_len = trg.size(0)\n",
        "        vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(src.device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        input_token = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(\n",
        "                input_token, hidden, cell, encoder_outputs\n",
        "            )\n",
        "\n",
        "            outputs[t] = output\n",
        "            best = output.argmax(1)\n",
        "\n",
        "            input_token = trg[t] if random.random() < teacher_forcing else best\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPoE3mOf0V7",
        "outputId": "15625fec-64ae-432c-8a53-28b31df1b248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:58<00:00,  5.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train=5.2803 | Val=5.0545\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:05<00:00,  4.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train=4.6113 | Val=4.4039\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:50<00:00,  5.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train=3.8322 | Val=3.7351\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [04:00<00:00,  3.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train=3.0802 | Val=3.1898\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:52<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train=2.5433 | Val=2.9880\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:28<00:00,  4.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train=2.1867 | Val=2.8308\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:02<00:00,  4.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train=1.9214 | Val=2.8322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:17<00:00,  6.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train=1.7069 | Val=2.9338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:22<00:00,  4.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train=1.5266 | Val=2.8485\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7. TRAINING + VALIDATION + EARLY STOPPING\n",
        "# ============================================================\n",
        "\n",
        "input_dim = len(en_vocab)\n",
        "output_dim = len(fr_vocab)\n",
        "\n",
        "encoder = Encoder(input_dim, 256, 512).to(device)\n",
        "decoder = Decoder(output_dim, 256, 512).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            outputs = model(src, trg, teacher_forcing=0)\n",
        "            loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                             trg[1:].reshape(-1))\n",
        "            total += loss.item()\n",
        "    return total / len(loader)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "best_val = float(\"inf\")\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, trg)\n",
        "\n",
        "        loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                         trg[1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train={train_loss:.4f} | Val={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BmW9W00XjCbs"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # tokenize câu tiếng Anh\n",
        "    tokens = en_tokenizer(sentence)\n",
        "\n",
        "    # chuyển sang ID\n",
        "    ids = (\n",
        "        [en_vocab.stoi[\"<sos>\"]] +\n",
        "        [en_vocab.stoi.get(t, en_vocab.stoi[\"<unk>\"]) for t in tokens] +\n",
        "        [en_vocab.stoi[\"<eos>\"]]\n",
        "    )\n",
        "\n",
        "    src = torch.tensor(ids).unsqueeze(1).to(device)   # shape: [seq_len, 1]\n",
        "\n",
        "    # ---- RUN ENCODER ----\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden, cell = model.encoder(src)\n",
        "\n",
        "    # bắt đầu decoder bằng token <sos>\n",
        "    input_tok = torch.tensor([fr_vocab.stoi[\"<sos>\"]]).to(device)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    # ---- RUN DECODER LOOP ----\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            pred, hidden, cell = model.decoder(\n",
        "                input_tok,\n",
        "                hidden,\n",
        "                cell,\n",
        "                encoder_outputs   # <<<< QUAN TRỌNG\n",
        "            )\n",
        "\n",
        "        top_id = pred.argmax(1).item()\n",
        "\n",
        "        if top_id == fr_vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "        outputs.append(top_id)\n",
        "        input_tok = torch.tensor([top_id]).to(device)\n",
        "\n",
        "    # chuyển ID → từ\n",
        "    return \" \".join(fr_vocab.itos[i] for i in outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_beam(sentence, max_len=50, beam_size=3):\n",
        "    model.eval()\n",
        "\n",
        "    # tokenize câu tiếng Anh\n",
        "    tokens = en_tokenizer(sentence)\n",
        "\n",
        "    # chuyển sang ID\n",
        "    ids = (\n",
        "        [en_vocab.stoi[\"<sos>\"]] +\n",
        "        [en_vocab.stoi.get(t, en_vocab.stoi[\"<unk>\"]) for t in tokens] +\n",
        "        [en_vocab.stoi[\"<eos>\"]]\n",
        "    )\n",
        "\n",
        "    src = torch.tensor(ids).unsqueeze(1).to(device)  # [seq_len, 1]\n",
        "\n",
        "    # ---- RUN ENCODER ----\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden, cell = model.encoder(src)\n",
        "\n",
        "    # beam = list of (sequence_ids, hidden, cell, score_log_prob)\n",
        "    beam = [([fr_vocab.stoi[\"<sos>\"]], hidden, cell, 0.0)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beam = []\n",
        "\n",
        "        for seq, h, c, score in beam:\n",
        "            input_tok = torch.tensor([seq[-1]]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred, h_new, c_new = model.decoder(input_tok, h, c, encoder_outputs)\n",
        "\n",
        "            log_probs = torch.log_softmax(pred, dim=1).squeeze(0)  # [vocab_size]\n",
        "\n",
        "            # lấy top k token\n",
        "            top_log_probs, top_ids = torch.topk(log_probs, beam_size)\n",
        "\n",
        "            for log_p, tok_id in zip(top_log_probs.tolist(), top_ids.tolist()):\n",
        "                new_seq = seq + [tok_id]\n",
        "                new_score = score + log_p\n",
        "                new_beam.append((new_seq, h_new, c_new, new_score))\n",
        "\n",
        "        # giữ lại beam_size sequences tốt nhất\n",
        "        new_beam = sorted(new_beam, key=lambda x: x[3], reverse=True)[:beam_size]\n",
        "        beam = new_beam\n",
        "\n",
        "        # nếu tất cả beam đã gặp <eos>, dừng\n",
        "        if all(seq[-1] == fr_vocab.stoi[\"<eos>\"] for seq, _, _, _ in beam):\n",
        "            break\n",
        "\n",
        "    # chọn sequence có score cao nhất\n",
        "    best_seq = beam[0][0]\n",
        "\n",
        "    # loại bỏ <sos> và cắt đến <eos>\n",
        "    if fr_vocab.stoi[\"<eos>\"] in best_seq:\n",
        "        eos_idx = best_seq.index(fr_vocab.stoi[\"<eos>\"])\n",
        "        best_seq = best_seq[1:eos_idx]\n",
        "    else:\n",
        "        best_seq = best_seq[1:]\n",
        "\n",
        "    return \" \".join(fr_vocab.itos[i] for i in best_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuSoBh6c4KzY",
        "outputId": "4710b255-5967-42af-80e0-f1787c17efb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus BLEU on Test = 0.23892762910101584\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_bleu():\n",
        "    references = []   # dạng: [[ref_tokens], [ref_tokens], ...]\n",
        "    hypotheses = []   # dạng: [pred_tokens, pred_tokens, ...]\n",
        "\n",
        "    for en, fr in zip(test_en_tok, test_fr_tok):\n",
        "        # input cho model là chuỗi tiếng Anh\n",
        "        pred = translate_beam(\" \".join(en))\n",
        "\n",
        "        # BLEU yêu cầu:\n",
        "        #   - ref: list các câu tham chiếu → mỗi câu phải bọc trong 1 list\n",
        "        #   - hyp: list các câu dự đoán tokenized\n",
        "        references.append([fr])\n",
        "        hypotheses.append(pred.split())\n",
        "\n",
        "    score = corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "        weights=(0.25, 0.25, 0.25, 0.25)\n",
        "    )\n",
        "    return score\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "bleu = evaluate_bleu()\n",
        "print(\"Corpus BLEU on Test =\", bleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "un jeune homme participe dans un fauteuil tandis que que a qui qui est\n",
            "un homme est la l' arrière dos son dos tout en regardant un livre dans un livre livre livre livre\n",
            "une personne portant des lunettes et un chapeau fait du\n",
            "un enfant est assis dans la rue dans une rue très fréquentée\n",
            "un chien brun avec une arrosoir trotte sur le\n",
            "un homme fait du vélo avec un enfant .\n"
          ]
        }
      ],
      "source": [
        "test = translate_beam(\"A young man participates in a career while the subject who records it smiles\")\n",
        "print(test)\n",
        "test = translate_beam(\"The man is scratching the back of his neck while looking for a book in a book store\")\n",
        "print(test)\n",
        "test = translate_beam(\"A person wearing goggles and a hat is sled riding\")\n",
        "print(test)\n",
        "test = translate_beam(\"A child sits on street on a busy street\")\n",
        "print(test)\n",
        "test = translate_beam(\"A brown dog with a watering can on the ocico\")\n",
        "print(test)\n",
        "test = translate_beam(\"A man is riding a bike with a child.\")\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1\n",
            "EN (Original) : a young man participates in a career while the subject who records it smiles .\n",
            "FR (Reference): un jeune homme participe à une course pendant que le sujet qui le filme sourit .\n",
            "FR (Predicted): un jeune homme jetant un snowboard tandis tandis tandis que le femme qui a être être . .\n",
            "------------------------------------------------------------\n",
            "Example 2\n",
            "EN (Original) : the man is scratching the back of his neck while looking for a book in a book store .\n",
            "FR (Reference): l' homme se gratte l' arrière du cou tout en cherchant un livre dans une librairie .\n",
            "FR (Predicted): l' homme est le l' de de sa tandis tandis que le livre dans un livre un un magasin .\n",
            "------------------------------------------------------------\n",
            "Example 3\n",
            "EN (Original) : a person wearing goggles and a hat is sled riding .\n",
            "FR (Reference): une personne portant des lunettes de protection et un chapeau fait de la luge .\n",
            "FR (Predicted): une personne avec des lunettes et un chapeau fait du snowboard .\n",
            "------------------------------------------------------------\n",
            "Example 4\n",
            "EN (Original) : a girl in a pink coat and flowered goloshes sledding down a hill .\n",
            "FR (Reference): une fille avec une veste rose et des galoches à fleurs descend le long d' une colline en luge .\n",
            "FR (Predicted): une fille avec un manteau rose et des baskets de dans une colline enneigée .\n",
            "------------------------------------------------------------\n",
            "Example 5\n",
            "EN (Original) : three girls are standing in front of a window of a building .\n",
            "FR (Reference): trois filles se tiennent devant la fenêtre d' un bâtiment .\n",
            "FR (Predicted): trois filles sont debout devant une fenêtre d' un bâtiment .\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Số câu muốn thử\n",
        "num_examples = 5  \n",
        "\n",
        "for i in range(num_examples):\n",
        "    # Lấy câu gốc tiếng Anh\n",
        "    en_sentence = \" \".join(test_en_tok[i])\n",
        "    \n",
        "    # Dịch sang tiếng Pháp\n",
        "    fr_pred = translate_beam(en_sentence, beam_size=3)\n",
        "    \n",
        "    # Lấy câu tham chiếu tiếng Pháp\n",
        "    fr_ref = \" \".join(test_fr_tok[i])\n",
        "    \n",
        "    # In ra kết quả\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"EN (Original) :\", en_sentence)\n",
        "    print(\"FR (Reference):\", fr_ref)\n",
        "    print(\"FR (Predicted):\", fr_pred)\n",
        "    print(\"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
