{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Y5-hTNuMCR",
        "outputId": "fe610795-e0c8-4324-f7f9-c5e284a8b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: spacy in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchtext) (2.32.5)\n",
            "Requirement already satisfied: numpy in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchtext) (2.2.3)\n",
            "Requirement already satisfied: six in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchtext) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchtext) (0.2.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.12.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.9.18)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->torchtext) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\dang khoa\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: wrapt in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.0)\n",
            "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
            "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.0/110.9 MB 7.5 MB/s eta 0:00:15\n",
            "   - -------------------------------------- 3.1/110.9 MB 8.7 MB/s eta 0:00:13\n",
            "   - -------------------------------------- 5.0/110.9 MB 8.6 MB/s eta 0:00:13\n",
            "   -- ------------------------------------- 6.8/110.9 MB 8.6 MB/s eta 0:00:13\n",
            "   --- ------------------------------------ 9.2/110.9 MB 9.2 MB/s eta 0:00:12\n",
            "   ---- ----------------------------------- 11.5/110.9 MB 9.6 MB/s eta 0:00:11\n",
            "   ---- ----------------------------------- 13.6/110.9 MB 9.7 MB/s eta 0:00:11\n",
            "   ----- ---------------------------------- 16.0/110.9 MB 9.8 MB/s eta 0:00:10\n",
            "   ------ --------------------------------- 18.6/110.9 MB 10.0 MB/s eta 0:00:10\n",
            "   ------- -------------------------------- 21.5/110.9 MB 10.5 MB/s eta 0:00:09\n",
            "   -------- ------------------------------- 24.1/110.9 MB 10.6 MB/s eta 0:00:09\n",
            "   --------- ------------------------------ 26.5/110.9 MB 10.5 MB/s eta 0:00:09\n",
            "   ---------- ----------------------------- 29.1/110.9 MB 10.7 MB/s eta 0:00:08\n",
            "   ----------- ---------------------------- 31.7/110.9 MB 10.9 MB/s eta 0:00:08\n",
            "   ------------ --------------------------- 34.9/110.9 MB 11.1 MB/s eta 0:00:07\n",
            "   ------------- -------------------------- 37.5/110.9 MB 11.2 MB/s eta 0:00:07\n",
            "   -------------- ------------------------- 39.8/110.9 MB 11.3 MB/s eta 0:00:07\n",
            "   --------------- ------------------------ 41.9/110.9 MB 11.3 MB/s eta 0:00:07\n",
            "   --------------- ------------------------ 43.3/110.9 MB 10.8 MB/s eta 0:00:07\n",
            "   ---------------- ----------------------- 45.1/110.9 MB 10.8 MB/s eta 0:00:07\n",
            "   ----------------- ---------------------- 47.2/110.9 MB 10.7 MB/s eta 0:00:06\n",
            "   ----------------- ---------------------- 49.0/110.9 MB 10.6 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 50.6/110.9 MB 10.5 MB/s eta 0:00:06\n",
            "   ------------------- -------------------- 53.2/110.9 MB 10.5 MB/s eta 0:00:06\n",
            "   -------------------- ------------------- 56.1/110.9 MB 10.6 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 58.5/110.9 MB 10.7 MB/s eta 0:00:05\n",
            "   ---------------------- ----------------- 61.3/110.9 MB 10.7 MB/s eta 0:00:05\n",
            "   ---------------------- ----------------- 63.7/110.9 MB 10.8 MB/s eta 0:00:05\n",
            "   ----------------------- ---------------- 66.1/110.9 MB 10.8 MB/s eta 0:00:05\n",
            "   ------------------------ --------------- 68.9/110.9 MB 10.9 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 71.8/110.9 MB 10.9 MB/s eta 0:00:04\n",
            "   -------------------------- ------------- 73.9/110.9 MB 10.9 MB/s eta 0:00:04\n",
            "   --------------------------- ------------ 76.0/110.9 MB 10.9 MB/s eta 0:00:04\n",
            "   ---------------------------- ----------- 77.9/110.9 MB 10.8 MB/s eta 0:00:04\n",
            "   ---------------------------- ----------- 80.2/110.9 MB 10.8 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 82.3/110.9 MB 10.8 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 84.4/110.9 MB 10.8 MB/s eta 0:00:03\n",
            "   ------------------------------- -------- 86.8/110.9 MB 10.8 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 88.9/110.9 MB 10.8 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 91.5/110.9 MB 10.8 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 93.8/110.9 MB 10.8 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 95.2/110.9 MB 10.7 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 97.3/110.9 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 98.8/110.9 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 99.6/110.9 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 99.6/110.9 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 99.6/110.9 MB 10.6 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 100.1/110.9 MB 9.9 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 102.2/110.9 MB 9.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 104.1/110.9 MB 9.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 106.2/110.9 MB 9.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 108.0/110.9 MB 9.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------  109.8/110.9 MB 9.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  110.9/110.9 MB 9.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 110.9/110.9 MB 9.6 MB/s eta 0:00:00\n",
            "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "Installing collected packages: torch, torchtext\n",
            "Successfully installed torch-2.9.1 torchtext-0.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\DANG KHOA\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\DANG KHOA\\AppData\\Local\\Programs\\Python\\Python313\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/12.8 MB 8.5 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 12.6 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 6.8/12.8 MB 13.5 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.7/12.8 MB 13.1 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 12.1/12.8 MB 13.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 12.5 MB/s  0:00:01\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/16.3 MB ? eta -:--:--\n",
            "     ----- ---------------------------------- 2.4/16.3 MB 9.0 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 4.2/16.3 MB 9.0 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 5.8/16.3 MB 8.6 MB/s eta 0:00:02\n",
            "     ------------------- -------------------- 7.9/16.3 MB 8.7 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 9.4/16.3 MB 8.8 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 11.5/16.3 MB 8.8 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 13.4/16.3 MB 8.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 15.5/16.3 MB 9.0 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 16.3/16.3 MB 8.8 MB/s  0:00:02\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext spacy nltk --user\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_L-aldtkrB",
        "outputId": "55e1da60-97ab-44c3-e62f-a9b8751a5ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import random\n",
        "# from numba import cuda\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2rjV_WtJE0_",
        "outputId": "f86fe5cb-56ed-4b02-bdee-4c5b7ef012e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 29000\n",
            "Val examples: 1014\n",
            "Test examples: 1071\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. LOAD DATA (TRAIN / VAL / TEST)\n",
        "# ============================================================\n",
        "\n",
        "def load_file(path):\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "# train_en = load_file(\"/content/train.en\")\n",
        "# train_fr = load_file(\"/content/train.fr\")\n",
        "\n",
        "# val_en = load_file(\"/content/val.en\")\n",
        "# val_fr = load_file(\"/content/val.fr\")\n",
        "\n",
        "# test_en = load_file(\"/content/test.en\")\n",
        "# test_fr = load_file(\"/content/test.fr\")\n",
        "\n",
        "train_en = load_file(\"./data/train.en\")\n",
        "train_fr = load_file(\"./data/train.fr\")\n",
        "\n",
        "val_en = load_file(\"./data/val.en\")\n",
        "val_fr = load_file(\"./data/val.fr\")\n",
        "\n",
        "test_en = load_file(\"./data/test.en\")\n",
        "test_fr = load_file(\"./data/test.fr\")\n",
        "\n",
        "print(\"Train examples:\", len(train_en))\n",
        "print(\"Val examples:\", len(val_en))\n",
        "print(\"Test examples:\", len(test_en))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "biwtBbHyK-xZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. TOKENIZATION (spaCy)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_en.tokenizer(text)]\n",
        "def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_fr.tokenizer(text)]\n",
        "\n",
        "train_en_tok = [en_tokenizer(s) for s in train_en]\n",
        "train_fr_tok = [fr_tokenizer(s) for s in train_fr]\n",
        "\n",
        "val_en_tok = [en_tokenizer(s) for s in val_en]\n",
        "val_fr_tok = [fr_tokenizer(s) for s in val_fr]\n",
        "\n",
        "test_en_tok = [en_tokenizer(s) for s in test_en]\n",
        "test_fr_tok = [fr_tokenizer(s) for s in test_fr]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-dQoFk3uW-mq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. BUILD VOCAB\n",
        "# ============================================================\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, max_size=10000):\n",
        "        self.freq = Counter(tokens)\n",
        "        most_common = self.freq.most_common(max_size)\n",
        "\n",
        "        # word2idx\n",
        "        self.itos = SPECIAL_TOKENS + [w for w, _ in most_common]\n",
        "        self.stoi = {w:i for i, w in enumerate(self.itos)}\n",
        "\n",
        "    def numericalize(self, tokens):\n",
        "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "en_vocab = Vocab(tok for sent in train_en_tok for tok in sent)\n",
        "fr_vocab = Vocab(tok for sent in train_fr_tok for tok in sent)\n",
        "\n",
        "PAD_IDX = fr_vocab.stoi[\"<pad>\"]\n",
        "SOS_IDX = fr_vocab.stoi[\"<sos>\"]\n",
        "EOS_IDX = fr_vocab.stoi[\"<eos>\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3aXkGj5CagMq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. DATASET + DATALOADER\n",
        "# ============================================================\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en_data, fr_data, vocab_en, vocab_fr):\n",
        "        self.en = en_data\n",
        "        self.fr = fr_data\n",
        "        self.vocab_en = vocab_en\n",
        "        self.vocab_fr = vocab_fr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "    # def numericalize(self, tokens, vocab):\n",
        "    #     return [vocab[\"<sos>\"]] + [vocab[t] for t in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     src_num = self.numericalize(self.src[idx], self.src_vocab)\n",
        "    #     trg_num = self.numericalize(self.trg[idx], self.trg_vocab)\n",
        "    #     return torch.tensor(src_num), torch.tensor(trg_num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_tokens = [\"<sos>\"] + self.en[idx] + [\"<eos>\"]\n",
        "        fr_tokens = [\"<sos>\"] + self.fr[idx] + [\"<eos>\"]\n",
        "\n",
        "        en_ids = self.vocab_en.numericalize(en_tokens)\n",
        "        fr_ids = self.vocab_fr.numericalize(fr_tokens)\n",
        "\n",
        "        return torch.tensor(en_ids), torch.tensor(fr_ids)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    en_batch, fr_batch = zip(*batch)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    return en_batch, fr_batch\n",
        "\n",
        "\n",
        "train_ds = TranslationDataset(train_en_tok, train_fr_tok, en_vocab, fr_vocab)\n",
        "val_ds = TranslationDataset(val_en_tok, val_fr_tok, en_vocab, fr_vocab)\n",
        "test_ds = TranslationDataset(test_en_tok, test_fr_tok, en_vocab, fr_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBKoX7BLcpDe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. ENCODER - DECODER MODEL\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing=0.3):\n",
        "        batch_size = trg.size(1)\n",
        "        max_len = trg.size(0)\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(src.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input_token = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            best = output.argmax(1)\n",
        "\n",
        "            input_token = trg[t] if random.random() < teacher_forcing else best\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPoE3mOf0V7",
        "outputId": "15625fec-64ae-432c-8a53-28b31df1b248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [19:49<00:00,  1.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train=4.6661 | Val=4.7307\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [12:25<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train=3.7345 | Val=4.3318\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:38<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train=3.3117 | Val=4.1014\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:32<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train=2.9872 | Val=3.9221\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:39<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train=2.7213 | Val=3.7534\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:33<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train=2.4848 | Val=3.7421\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:34<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train=2.2701 | Val=3.6679\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:31<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train=2.0794 | Val=3.6034\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [12:00<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train=1.9180 | Val=3.5917\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:30<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train=1.7467 | Val=3.6811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:24<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Train=1.6129 | Val=3.6547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [11:27<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Train=1.4812 | Val=3.6426\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7. TRAINING + VALIDATION + EARLY STOPPING\n",
        "# ============================================================\n",
        "\n",
        "input_dim = len(en_vocab)\n",
        "output_dim = len(fr_vocab)\n",
        "\n",
        "encoder = Encoder(input_dim, 256, 512).to(device)\n",
        "decoder = Decoder(output_dim, 256, 512).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            outputs = model(src, trg, teacher_forcing=0.3)\n",
        "            loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                             trg[1:].reshape(-1))\n",
        "            total += loss.item()\n",
        "    return total / len(loader)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "best_val = float(\"inf\")\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, trg)\n",
        "\n",
        "        loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                         trg[1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train={train_loss:.4f} | Val={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmW9W00XjCbs"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    model.eval()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = en_tokenizer(sentence)\n",
        "\n",
        "    # tạo vector số hóa đúng chuẩn\n",
        "    ids = (\n",
        "        [en_vocab.stoi[\"<sos>\"]] +\n",
        "        [en_vocab.stoi.get(t, en_vocab.stoi[\"<unk>\"]) for t in tokens] +\n",
        "        [en_vocab.stoi[\"<eos>\"]]\n",
        "    )\n",
        "\n",
        "    src = torch.tensor(ids).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src)\n",
        "\n",
        "    # token đầu tiên đầu ra\n",
        "    input_tok = torch.tensor([fr_vocab.stoi[\"<sos>\"]]).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for _ in range(50):\n",
        "        with torch.no_grad():\n",
        "            pred, hidden, cell = model.decoder(input_tok, hidden, cell)\n",
        "\n",
        "        top_id = pred.argmax(1).item()\n",
        "\n",
        "        if top_id == fr_vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "        outputs.append(top_id)\n",
        "        input_tok = torch.tensor([top_id]).to(device)\n",
        "\n",
        "    return \" \".join(fr_vocab.itos[i] for i in outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuSoBh6c4KzY",
        "outputId": "4710b255-5967-42af-80e0-f1787c17efb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus BLEU on Test = 0.14282269136959508\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_bleu():\n",
        "    references = []   # dạng: [[ref_tokens], [ref_tokens], ...]\n",
        "    hypotheses = []   # dạng: [pred_tokens, pred_tokens, ...]\n",
        "\n",
        "    for en, fr in zip(test_en_tok, test_fr_tok):\n",
        "        # input cho model là chuỗi tiếng Anh\n",
        "        pred = translate(\" \".join(en))\n",
        "\n",
        "        # BLEU yêu cầu:\n",
        "        #   - ref: list các câu tham chiếu → mỗi câu phải bọc trong 1 list\n",
        "        #   - hyp: list các câu dự đoán tokenized\n",
        "        references.append([fr])\n",
        "        hypotheses.append(pred.split())\n",
        "\n",
        "    score = corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "        weights=(0.25, 0.25, 0.25, 0.25)\n",
        "    )\n",
        "    return score\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "bleu = evaluate_bleu()\n",
        "print(\"Corpus BLEU on Test =\", bleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwwrZrR9Ndrl",
        "outputId": "0a13bd3c-31d7-4d8d-b744-d49baf6884a2"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "test = translate(\"I go to school every day.\")\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9CU__D9C121"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# # Giữ nguyên test_en, test_de\n",
        "# hyps = []\n",
        "# refs = []\n",
        "\n",
        "# for src_sent, trg_sent in zip(test_en_tok, test_fr_tok):\n",
        "#     hyp = translate(src_sent).split()  # dự đoán tiếng Đức\n",
        "#     refs.append([fr_tokenizer(trg_sent)])              # reference tiếng Đức\n",
        "#     hyps.append(hyp)\n",
        "\n",
        "# bleu = corpus_bleu(refs, hyps)\n",
        "# print(\"BLEU (corpus) on test set:\", bleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
