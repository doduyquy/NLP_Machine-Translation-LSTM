{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.0+cu121\n",
            "True\n",
            "NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1Y5-hTNuMCR",
        "outputId": "fe610795-e0c8-4324-f7f9-c5e284a8b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (3.9.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: setuptools in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.12.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: joblib in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: click in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dang khoa\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in c:\\users\\dang khoa\\appdata\\roaming\\python\\python310\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/12.8 MB 2.0 MB/s eta 0:00:07\n",
            "      --------------------------------------- 0.2/12.8 MB 2.8 MB/s eta 0:00:05\n",
            "     - -------------------------------------- 0.5/12.8 MB 4.3 MB/s eta 0:00:03\n",
            "     -- ------------------------------------- 0.9/12.8 MB 5.9 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.4/12.8 MB 6.2 MB/s eta 0:00:02\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 7.0 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 2.2/12.8 MB 7.3 MB/s eta 0:00:02\n",
            "     ------- -------------------------------- 2.5/12.8 MB 7.4 MB/s eta 0:00:02\n",
            "     --------- ------------------------------ 3.0/12.8 MB 7.5 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 7.8 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 3.8/12.8 MB 7.8 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 4.1/12.8 MB 7.8 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 4.6/12.8 MB 7.9 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 5.0/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ---------------- ----------------------- 5.4/12.8 MB 8.0 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 5.8/12.8 MB 8.0 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 6.2/12.8 MB 8.0 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 6.6/12.8 MB 8.1 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 6.9/12.8 MB 8.0 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 7.2/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.5/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.5/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 7.5/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 8.6/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 8.9/12.8 MB 7.8 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 9.3/12.8 MB 7.8 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 9.7/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 10.1/12.8 MB 7.9 MB/s eta 0:00:01\n",
            "     -------------------------------- ------- 10.5/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 10.9/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.8/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.5/12.8 MB 8.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.8/12.8 MB 8.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 7.8 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.1/16.3 MB 2.6 MB/s eta 0:00:07\n",
            "      --------------------------------------- 0.3/16.3 MB 3.1 MB/s eta 0:00:06\n",
            "     - -------------------------------------- 0.6/16.3 MB 4.7 MB/s eta 0:00:04\n",
            "     -- ------------------------------------- 1.0/16.3 MB 5.8 MB/s eta 0:00:03\n",
            "     -- ------------------------------------- 1.2/16.3 MB 5.3 MB/s eta 0:00:03\n",
            "     ---- ----------------------------------- 1.8/16.3 MB 6.6 MB/s eta 0:00:03\n",
            "     ----- ---------------------------------- 2.4/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     ------ --------------------------------- 2.8/16.3 MB 7.5 MB/s eta 0:00:02\n",
            "     -------- ------------------------------- 3.5/16.3 MB 8.2 MB/s eta 0:00:02\n",
            "     ---------- ----------------------------- 4.1/16.3 MB 9.0 MB/s eta 0:00:02\n",
            "     ----------- ---------------------------- 4.5/16.3 MB 9.0 MB/s eta 0:00:02\n",
            "     ------------ --------------------------- 5.0/16.3 MB 9.1 MB/s eta 0:00:02\n",
            "     ------------- -------------------------- 5.6/16.3 MB 9.3 MB/s eta 0:00:02\n",
            "     -------------- ------------------------- 6.0/16.3 MB 9.3 MB/s eta 0:00:02\n",
            "     --------------- ------------------------ 6.5/16.3 MB 9.4 MB/s eta 0:00:02\n",
            "     ---------------- ----------------------- 6.9/16.3 MB 9.4 MB/s eta 0:00:02\n",
            "     ----------------- ---------------------- 7.3/16.3 MB 9.3 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 7.5/16.3 MB 9.0 MB/s eta 0:00:01\n",
            "     ------------------ --------------------- 7.7/16.3 MB 8.8 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 7.9/16.3 MB 8.6 MB/s eta 0:00:01\n",
            "     ------------------- -------------------- 8.1/16.3 MB 8.5 MB/s eta 0:00:01\n",
            "     -------------------- ------------------- 8.3/16.3 MB 8.2 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 8.6/16.3 MB 8.1 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 8.8/16.3 MB 7.9 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 9.1/16.3 MB 7.8 MB/s eta 0:00:01\n",
            "     ---------------------- ----------------- 9.3/16.3 MB 7.7 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 9.5/16.3 MB 7.6 MB/s eta 0:00:01\n",
            "     ----------------------- ---------------- 9.7/16.3 MB 7.5 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 9.9/16.3 MB 7.4 MB/s eta 0:00:01\n",
            "     ------------------------ --------------- 10.1/16.3 MB 7.3 MB/s eta 0:00:01\n",
            "     ------------------------- -------------- 10.4/16.3 MB 7.4 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 10.7/16.3 MB 7.5 MB/s eta 0:00:01\n",
            "     -------------------------- ------------- 10.9/16.3 MB 7.4 MB/s eta 0:00:01\n",
            "     --------------------------- ------------ 11.2/16.3 MB 7.3 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 11.5/16.3 MB 7.3 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 11.7/16.3 MB 7.1 MB/s eta 0:00:01\n",
            "     ----------------------------- ---------- 12.0/16.3 MB 7.0 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 12.2/16.3 MB 6.9 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 12.4/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 12.9/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     --------------------------------- ------ 13.5/16.3 MB 6.8 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 13.9/16.3 MB 6.7 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 14.4/16.3 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 14.7/16.3 MB 6.5 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 15.1/16.3 MB 6.6 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 15.4/16.3 MB 6.5 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 15.7/16.3 MB 6.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  16.0/16.3 MB 6.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------  16.3/16.3 MB 6.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 16.3/16.3 MB 6.2 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy nltk --user\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_L-aldtkrB",
        "outputId": "55e1da60-97ab-44c3-e62f-a9b8751a5ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1. IMPORTS\n",
        "# ============================================================\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import random\n",
        "# from numba import cuda\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2rjV_WtJE0_",
        "outputId": "f86fe5cb-56ed-4b02-bdee-4c5b7ef012e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 29000\n",
            "Val examples: 1014\n",
            "Test examples: 1071\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. LOAD DATA (TRAIN / VAL / TEST)\n",
        "# ============================================================\n",
        "\n",
        "def load_file(path):\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "# train_en = load_file(\"/content/train.en\")\n",
        "# train_fr = load_file(\"/content/train.fr\")\n",
        "\n",
        "# val_en = load_file(\"/content/val.en\")\n",
        "# val_fr = load_file(\"/content/val.fr\")\n",
        "\n",
        "# test_en = load_file(\"/content/test.en\")\n",
        "# test_fr = load_file(\"/content/test.fr\")\n",
        "\n",
        "train_en = load_file(\"./data/train.en\")\n",
        "train_fr = load_file(\"./data/train.fr\")\n",
        "\n",
        "val_en = load_file(\"./data/val.en\")\n",
        "val_fr = load_file(\"./data/val.fr\")\n",
        "\n",
        "test_en = load_file(\"./data/test.en\")\n",
        "test_fr = load_file(\"./data/test.fr\")\n",
        "\n",
        "print(\"Train examples:\", len(train_en))\n",
        "print(\"Val examples:\", len(val_en))\n",
        "print(\"Test examples:\", len(test_en))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "biwtBbHyK-xZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. TOKENIZATION (spaCy)\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_en.tokenizer(text)]\n",
        "def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in nlp_fr.tokenizer(text)]\n",
        "\n",
        "train_en_tok = [en_tokenizer(s) for s in train_en]\n",
        "train_fr_tok = [fr_tokenizer(s) for s in train_fr]\n",
        "\n",
        "val_en_tok = [en_tokenizer(s) for s in val_en]\n",
        "val_fr_tok = [fr_tokenizer(s) for s in val_fr]\n",
        "\n",
        "test_en_tok = [en_tokenizer(s) for s in test_en]\n",
        "test_fr_tok = [fr_tokenizer(s) for s in test_fr]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-dQoFk3uW-mq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. BUILD VOCAB\n",
        "# ============================================================\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, max_size=10000):\n",
        "        self.freq = Counter(tokens)\n",
        "        most_common = self.freq.most_common(max_size)\n",
        "\n",
        "        # word2idx\n",
        "        self.itos = SPECIAL_TOKENS + [w for w, _ in most_common]\n",
        "        self.stoi = {w:i for i, w in enumerate(self.itos)}\n",
        "\n",
        "    def numericalize(self, tokens):\n",
        "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "en_vocab = Vocab(tok for sent in train_en_tok for tok in sent)\n",
        "fr_vocab = Vocab(tok for sent in train_fr_tok for tok in sent)\n",
        "\n",
        "PAD_IDX = fr_vocab.stoi[\"<pad>\"]\n",
        "SOS_IDX = fr_vocab.stoi[\"<sos>\"]\n",
        "EOS_IDX = fr_vocab.stoi[\"<eos>\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3aXkGj5CagMq"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. DATASET + DATALOADER\n",
        "# ============================================================\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en_data, fr_data, vocab_en, vocab_fr):\n",
        "        self.en = en_data\n",
        "        self.fr = fr_data\n",
        "        self.vocab_en = vocab_en\n",
        "        self.vocab_fr = vocab_fr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "    # def numericalize(self, tokens, vocab):\n",
        "    #     return [vocab[\"<sos>\"]] + [vocab[t] for t in tokens] + [vocab[\"<eos>\"]]\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     src_num = self.numericalize(self.src[idx], self.src_vocab)\n",
        "    #     trg_num = self.numericalize(self.trg[idx], self.trg_vocab)\n",
        "    #     return torch.tensor(src_num), torch.tensor(trg_num)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_tokens = [\"<sos>\"] + self.en[idx] + [\"<eos>\"]\n",
        "        fr_tokens = [\"<sos>\"] + self.fr[idx] + [\"<eos>\"]\n",
        "\n",
        "        en_ids = self.vocab_en.numericalize(en_tokens)\n",
        "        fr_ids = self.vocab_fr.numericalize(fr_tokens)\n",
        "\n",
        "        return torch.tensor(en_ids), torch.tensor(fr_ids)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    en_batch, fr_batch = zip(*batch)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
        "    return en_batch, fr_batch\n",
        "\n",
        "\n",
        "train_ds = TranslationDataset(train_en_tok, train_fr_tok, en_vocab, fr_vocab)\n",
        "val_ds = TranslationDataset(val_en_tok, val_fr_tok, en_vocab, fr_vocab)\n",
        "test_ds = TranslationDataset(test_en_tok, test_fr_tok, en_vocab, fr_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iBKoX7BLcpDe"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. ENCODER - DECODER MODEL\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing=0.3):\n",
        "        batch_size = trg.size(1)\n",
        "        max_len = trg.size(0)\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, vocab_size).to(src.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "        input_token = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            best = output.argmax(1)\n",
        "\n",
        "            input_token = trg[t] if random.random() < teacher_forcing else best\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPoE3mOf0V7",
        "outputId": "15625fec-64ae-432c-8a53-28b31df1b248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:24<00:00,  6.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train=4.8306 | Val=4.1716\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:47<00:00,  5.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train=3.9689 | Val=3.6688\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:52<00:00,  5.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train=3.5240 | Val=3.3385\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:02<00:00,  4.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train=3.1753 | Val=3.1139\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:50<00:00,  5.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train=2.8826 | Val=2.9163\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:00<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Train=2.6448 | Val=2.9446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:10<00:00,  6.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Train=2.4218 | Val=2.8927\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [01:47<00:00,  8.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Train=2.2174 | Val=2.8010\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [03:03<00:00,  4.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Train=2.0426 | Val=2.7770\n",
            "Saved best model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:32<00:00,  5.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Train=1.8803 | Val=2.8363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:40<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Train=1.7231 | Val=2.8413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 907/907 [02:46<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Train=1.5862 | Val=2.8646\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7. TRAINING + VALIDATION + EARLY STOPPING\n",
        "# ============================================================\n",
        "\n",
        "input_dim = len(en_vocab)\n",
        "output_dim = len(fr_vocab)\n",
        "\n",
        "encoder = Encoder(input_dim, 256, 512).to(device)\n",
        "decoder = Decoder(output_dim, 256, 512).to(device)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            outputs = model(src, trg, teacher_forcing=0.3)\n",
        "            loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                             trg[1:].reshape(-1))\n",
        "            total += loss.item()\n",
        "    return total / len(loader)\n",
        "\n",
        "\n",
        "EPOCHS = 20\n",
        "best_val = float(\"inf\")\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, trg)\n",
        "\n",
        "        loss = criterion(outputs[1:].reshape(-1, outputs.size(-1)),\n",
        "                         trg[1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train={train_loss:.4f} | Val={val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Saved best model\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BmW9W00XjCbs"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    model.eval()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = en_tokenizer(sentence)\n",
        "\n",
        "    # tạo vector số hóa đúng chuẩn\n",
        "    ids = (\n",
        "        [en_vocab.stoi[\"<sos>\"]] +\n",
        "        [en_vocab.stoi.get(t, en_vocab.stoi[\"<unk>\"]) for t in tokens] +\n",
        "        [en_vocab.stoi[\"<eos>\"]]\n",
        "    )\n",
        "\n",
        "    src = torch.tensor(ids).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src)\n",
        "\n",
        "    # token đầu tiên đầu ra\n",
        "    input_tok = torch.tensor([fr_vocab.stoi[\"<sos>\"]]).to(device)\n",
        "    outputs = []\n",
        "\n",
        "    for _ in range(50):\n",
        "        with torch.no_grad():\n",
        "            pred, hidden, cell = model.decoder(input_tok, hidden, cell)\n",
        "\n",
        "        top_id = pred.argmax(1).item()\n",
        "\n",
        "        if top_id == fr_vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "        outputs.append(top_id)\n",
        "        input_tok = torch.tensor([top_id]).to(device)\n",
        "\n",
        "    return \" \".join(fr_vocab.itos[i] for i in outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuSoBh6c4KzY",
        "outputId": "4710b255-5967-42af-80e0-f1787c17efb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus BLEU on Test = 0.14313889922319398\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_bleu():\n",
        "    references = []   # dạng: [[ref_tokens], [ref_tokens], ...]\n",
        "    hypotheses = []   # dạng: [pred_tokens, pred_tokens, ...]\n",
        "\n",
        "    for en, fr in zip(test_en_tok, test_fr_tok):\n",
        "        # input cho model là chuỗi tiếng Anh\n",
        "        pred = translate(\" \".join(en))\n",
        "\n",
        "        # BLEU yêu cầu:\n",
        "        #   - ref: list các câu tham chiếu → mỗi câu phải bọc trong 1 list\n",
        "        #   - hyp: list các câu dự đoán tokenized\n",
        "        references.append([fr])\n",
        "        hypotheses.append(pred.split())\n",
        "\n",
        "    score = corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "    )\n",
        "    return score\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "bleu = evaluate_bleu()\n",
        "print(\"Corpus BLEU on Test =\", bleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1\n",
            "EN (Original) : a young man participates in a career while the subject who records it smiles .\n",
            "FR (Reference): un jeune homme participe à une course pendant que le sujet qui le filme sourit .\n",
            "FR (Predicted): un jeune homme est dans un miroir tandis que il est lui lui lui .\n",
            "------------------------------------------------------------\n",
            "Example 2\n",
            "EN (Original) : the man is scratching the back of his neck while looking for a book in a book store .\n",
            "FR (Reference): l' homme se gratte l' arrière du cou tout en cherchant un livre dans une librairie .\n",
            "FR (Predicted): l' homme examine le visage de la tête tandis qu' un un un un un un un un un .\n",
            "------------------------------------------------------------\n",
            "Example 3\n",
            "EN (Original) : a person wearing goggles and a hat is sled riding .\n",
            "FR (Reference): une personne portant des lunettes de protection et un chapeau fait de la luge .\n",
            "FR (Predicted): une personne portant des lunettes et un casque fait fait du vélo .\n",
            "------------------------------------------------------------\n",
            "Example 4\n",
            "EN (Original) : a girl in a pink coat and flowered goloshes sledding down a hill .\n",
            "FR (Reference): une fille avec une veste rose et des galoches à fleurs descend le long d' une colline en luge .\n",
            "FR (Predicted): une fille en une rose et et tongs fait du yoga dans une une .\n",
            "------------------------------------------------------------\n",
            "Example 5\n",
            "EN (Original) : three girls are standing in front of a window of a building .\n",
            "FR (Reference): trois filles se tiennent devant la fenêtre d' un bâtiment .\n",
            "FR (Predicted): trois filles sont debout devant un bâtiment d' un bâtiment .\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Số câu muốn thử\n",
        "num_examples = 5\n",
        "\n",
        "for i in range(num_examples):\n",
        "    # Lấy câu gốc tiếng Anh\n",
        "    en_sentence = \" \".join(test_en_tok[i])\n",
        "\n",
        "    # Dịch sang tiếng Pháp\n",
        "    fr_pred = translate(en_sentence)\n",
        "\n",
        "    # Lấy câu tham chiếu tiếng Pháp\n",
        "    fr_ref = \" \".join(test_fr_tok[i])\n",
        "\n",
        "    # In ra kết quả\n",
        "    print(f\"Example {i+1}\")\n",
        "    print(\"EN (Original) :\", en_sentence)\n",
        "    print(\"FR (Reference):\", fr_ref)\n",
        "    print(\"FR (Predicted):\", fr_pred)\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwwrZrR9Ndrl",
        "outputId": "0a13bd3c-31d7-4d8d-b744-d49baf6884a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "un jeune homme est dans un miroir tandis que il a ses cheveux .\n",
            "l' homme se du la de la tête tandis qu' un un un un un un un un un .\n"
          ]
        }
      ],
      "source": [
        "test = translate(\"A young man participates in a career while the subject who records it smiles\")\n",
        "print(test)\n",
        "test = translate(\"The man is scratching the back of his neck while looking for a book in a book store\")\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e9CU__D9C121"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# # Giữ nguyên test_en, test_de\n",
        "# hyps = []\n",
        "# refs = []\n",
        "\n",
        "# for src_sent, trg_sent in zip(test_en_tok, test_fr_tok):\n",
        "#     hyp = translate(src_sent).split()  # dự đoán tiếng Đức\n",
        "#     refs.append([fr_tokenizer(trg_sent)])              # reference tiếng Đức\n",
        "#     hyps.append(hyp)\n",
        "\n",
        "# bleu = corpus_bleu(refs, hyps)\n",
        "# print(\"BLEU (corpus) on test set:\", bleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
